from airflow.decorators import dag, task
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.operators.bash import BashOperator
import pendulum
import pandas as pd

# ðŸŒŸ 1. dbt í”„ë¡œì íŠ¸ê°€ ìœ„ì¹˜í•œ ê²½ë¡œ (Airflow ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
DBT_PROJECT_DIR = "/opt/airflow/dbt"
# ðŸŒŸ 2. CSV íŒŒì¼ì´ ìœ„ì¹˜í•œ ê²½ë¡œ (Airflow ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
DATA_FILE_PATH = "/opt/airflow/data/patient_treatment.csv"
# ðŸŒŸ 3. ì ìž¬í•  DBì˜ Connection ID (Airflow UIì—ì„œ ë§Œë“¤ í•„ìš” ì—†ìŒ. Hookì´ ì§ì ‘ ì—°ê²°)
POSTGRES_CONN_ID = "de_project_postgres" # docker-compose.ymlì˜ ì„œë¹„ìŠ¤ ì´ë¦„ ì‚¬ìš©

@dag(
    dag_id="elt_patient_pipeline",
    start_date=pendulum.datetime(2023, 1, 1, tz="Asia/Seoul"),
    schedule=None,
    catchup=False,
)
def elt_patient_pipeline():
    """
    [2ë‹¨ê³„] ELT íŒŒì´í”„ë¼ì¸ (Airflow + dbt)
    1. (EL) CSV íŒŒì¼ì„ Pandasë¡œ ì½ì–´ Postgres(analytics_db)ì— ì ìž¬
    2. (T) dbtë¥¼ ì‹¤í–‰í•˜ì—¬ ì ìž¬ëœ ë°ì´í„°ë¥¼ ë³€í™˜
    """

    @task
    def load_csv_to_postgres():
        """
        [E + L] CSV íŒŒì¼ì„ Pandasë¡œ ì½ì–´ Postgres 'raw.raw_patient_events' í…Œì´ë¸”ì— ì ìž¬
        """
        # 1. CSV íŒŒì¼ ì½ê¸°
        df = pd.read_csv(DATA_FILE_PATH, header=0)
        
        # 2. PostgresHookì„ ì‚¬ìš©í•˜ì—¬ DB ì—°ê²° (dbnameì„ 'analytics_db'ë¡œ ì§€ì •)
        hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID, database="analytics_db")
        
        # 3. 'raw' ìŠ¤í‚¤ë§ˆê°€ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
        hook.run("CREATE SCHEMA IF NOT EXISTS raw;")
        
        # 4. ðŸŒŸ CASCADE ì˜µì…˜ìœ¼ë¡œ í…Œì´ë¸” ë° ì˜ì¡´ ê°ì²´(ë·°)ë¥¼ ê°•ì œ ì‚­ì œí•©ë‹ˆë‹¤.
        hook.run("DROP TABLE IF EXISTS raw.raw_patient_events CASCADE;")

        # 5. hookì—ì„œ SQLAlchemy ì—”ì§„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        engine = hook.get_sqlalchemy_engine()

        # 6. ðŸŒŸ pandas.to_sqlì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        #    if_exists='append' : 4ë²ˆì—ì„œ í…Œì´ë¸”ì„ í™•ì‹¤ížˆ ì§€ì› ìœ¼ë¯€ë¡œ,
        #                       í…Œì´ë¸”ì„ ìƒˆë¡œ ìƒì„±í•˜ê³  ë°ì´í„°ë¥¼ 'append' (ì¶”ê°€)í•©ë‹ˆë‹¤.
        df.to_sql(
            name="raw_patient_events",  # í…Œì´ë¸” ì´ë¦„
            con=engine,                 # SQLAlchemy ì—”ì§„
            schema="raw",               # ìŠ¤í‚¤ë§ˆ ì´ë¦„
            if_exists="append",         # ðŸŒŸ 'replace' -> 'append'ë¡œ ë³€ê²½
            index=False                 # Pandasì˜ ì¸ë±ìŠ¤ëŠ” ì €ìž¥í•˜ì§€ ì•ŠìŒ
        )
        
        print(f"Successfully loaded {len(df)} rows to raw.raw_patient_events")

    # [T] dbt ì‹¤í–‰ (BashOperator ì‚¬ìš©)
    run_dbt = BashOperator(
        task_id="run_dbt",
        # ðŸŒŸ dbt í”„ë¡œì íŠ¸ í´ë”ë¡œ ì´ë™í•˜ì—¬ 'dbt run'ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        bash_command=f"cd {DBT_PROJECT_DIR} && dbt run"
    )

    # íŒŒì´í”„ë¼ì¸ ìˆœì„œ ì„¤ì •: load íƒœìŠ¤í¬ê°€ ì„±ê³µí•´ì•¼ run_dbt íƒœìŠ¤í¬ê°€ ì‹¤í–‰ë¨
    load_csv_to_postgres() >> run_dbt

# DAG ì‹¤í–‰
elt_patient_pipeline()