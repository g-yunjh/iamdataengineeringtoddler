# Step 2: Airflow와 dbt로 ELT 파이프라인 구축하기

1단계에서 준비된 환경을 바탕으로, 실제 데이터 파이프라인을 구축합니다. 이 파이프라인은 `patient_treatment.csv` 파일을 DB에 적재(EL)하고, dbt를 사용해 변환(T)하는 작업을 Airflow로 자동화합니다.

## 흐름

1.  **Extract / Load (Airflow - Python):**
    `patient_treatment.csv` 파일을 `pandas`로 읽어 `analytics_db`의 `raw.raw_patient_events` 테이블에 적재합니다.

2.  **Transform (Airflow - Bash/dbt):**
    Airflow가 `dbt run` 명령어를 실행하여 `raw.raw_patient_events`를 참조하는 dbt 모델(SQL)을 실행, `mart.mart_patient_summary` 테이블을 생성합니다.

## 1. (T) dbt 프로젝트 설정

파이프라인의 'T' 부분을 담당하는 dbt 프로젝트를 설정합니다.

* **`dbt/profiles.yml`:**
    * dbt가 어떤 DB에 연결해야 하는지 정의하는 파일입니다.
    * `host: postgres` (Docker 네트워크의 서비스 이름)
    * `dbname: analytics_db` (1단계에서 생성한 웨어하우스 DB)
    * `schema: public` (필수 항목이라 추가. dbt의 기본 스키마)

* **`dbt/dbt_project.yml`:**
    * `staging`, `marts` 등 dbt 모델이 저장될 경로와 스키마를 정의합니다.
    * e.g., `marts` 폴더의 모델들은 `+schema: mart` 설정에 따라 `mart` 스키마에 생성됩니다.

* **`dbt/macros/get_schema_name.sql` (중요):**
    * dbt는 기본적으로 `(기본 스키마)_(커스텀 스키마)` (e.g., `public_mart`)로 스키마 이름을 만듭니다.
    * 이 매크로는 그 규칙을 무시하고 `dbt_project.yml`에 정의된 커스텀 스키마 이름(e.g., `mart`)을 그대로 사용하도록 강제합니다.

* **`dbt/models`:**
    * `staging/stg_patient_events.sql`: `raw.raw_patient_events`를 참조(`{{ source(...) }}`)하여 컬럼명을 변경하고 타입을 캐스팅하는 등 1차 정제 작업을 합니다.
    * `marts/mart_patient_summary.sql`: `stg_patient_events`를 참조(`{{ ref(...) }}`)하여 환자별 방문 횟수 등을 집계하는 최종 분석용 테이블을 만듭니다.

## 2. (ELT) Airflow DAG 작성

이 모든 과정을 지휘하는 `dags/elt_patient_pipeline.py` 파일을 작성합니다.

* **Airflow Connection 설정 (필수):**
    * DAG가 `PostgresHook`을 통해 DB에 접속하려면 Airflow가 접속 정보를 알아야 합니다.
    * Airflow UI > Admin > Connections에서 `de_project_postgres`라는 이름으로 `host: postgres`, `login: airflow` 등의 연결 정보를 미리 생성해야 합니다.

* **Task 1: `load_csv_to_postgres` (Python)**
    * `@task` 데코레이터를 사용한 Python 함수입니다.
    * 이 작업은 여러 번의 디버깅을 거쳐 최종적으로 다음과 같은 로직을 갖습니다.
    * **1. `hook.run("CREATE SCHEMA IF NOT EXISTS raw;")`:** `pandas.to_sql`이 스키마를 자동으로 만들지 않으므로, 스키마를 먼저 생성합니다.
    * **2. `hook.run("DROP TABLE IF EXISTS raw.raw_patient_events CASCADE;")`:**
        * 이 파이프라인이 여러 번 실행될 수 있도록 테이블을 삭제합니다.
        * **`CASCADE`** 옵션이 핵심입니다. dbt가 이 테이블을 참조하는 `stg_patient_events` **뷰(View)**를 생성했기 때문에, `CASCADE` 없이는 의존성 문제로 테이블이 삭제되지 않습니다.
    * **3. `pandas.to_sql(if_exists="append", ...)`:**
        * `if_exists="replace"`는 `CASCADE` 기능이 없어 의존성 에러가 발생했습니다.
        * `DROP ... CASCADE`로 테이블을 확실히 지웠으므로, 새로 생성된 빈 테이블에 `append` (추가)하는 방식으로 데이터를 적재합니다.

* **Task 2: `run_dbt` (Bash)**
    * `BashOperator`를 사용해 dbt 프로젝트 폴더로 이동한 뒤 `dbt run` 명령어를 실행합니다.

* **태스크 순서:**
    * `load_csv_to_postgres() >> run_dbt`
    * (E+L) 작업이 성공해야 (T) 작업이 실행되도록 순서를 지정합니다.

## 3. 실행 및 확인

1.  Airflow UI (`localhost:8080`)에서 `elt_patient_pipeline` DAG를 찾아 토글을 켜고 실행합니다.
2.  두 태스크가 모두 초록색(성공)이 되면, 터미널에서 `psql`로 접속해 `mart.mart_patient_summary` 테이블에 데이터가 잘 쌓였는지 확인합니다.