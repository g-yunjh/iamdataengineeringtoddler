# Step 3: PySpark를 활용한 대용량 배치 처리 학습

## 1\. 학습 목표

  * 기존 Pandas(Single-node) 기반의 처리를 **분산 처리 시스템인 Apache Spark**로 전환.
  * Airflow와 Spark Cluster(Master/Worker)를 연동하여 배치 파이프라인 구축.

## 2\. 아키텍처 구조 (Spark on Docker)

  * **Cluster Mode:** Standalone Mode 사용 (Docker로 Master 1대, Worker 1대 구성).
  * **Driver:** 작업을 제출하고 제어하는 주체. 여기서는 **Airflow Scheduler**가 Driver 역할을 수행 (`Client Mode`).
  * **Executor:** 실제 연산을 수행하는 프로세스. Spark Worker 컨테이너 내에서 실행됨.

## 3\. PySpark 핵심 개념 및 구현

### 3.1 SparkSession (진입점)

Spark 2.0부터는 `SparkConf`, `SparkContext` 대신 `SparkSession` 하나로 통합됨.

  * **JDBC 드라이버 로드:** Postgres 연결을 위해 `config("spark.jars.packages", ...)`로 드라이버를 런타임에 다운로드함.

<!-- end list -->

```python
spark = SparkSession.builder \
    .appName("Patient_Data_Loader") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.6.0") \
    .getOrCreate()
```

### 3.2 DataFrame과 Lazy Evaluation (지연 연산)

  * **DataFrame:** RDD 기반의 고수준 API. Pandas와 비슷하지만 데이터가 분산되어 있음.
  * **Lazy Evaluation:** `spark.read.csv`나 `df.select`를 호출할 때 바로 실행되지 않음. `write`나 `show` 같은 **Action**이 일어날 때 비로소 실행 계획(DAG)을 만들고 처리함.

### 3.3 데이터 적재 (JDBC)

  * `mode("overwrite")`: 테이블이 있으면 `DROP` 후 `CREATE`.
  * **주의사항:** Spark의 overwrite는 `CASCADE` 옵션이 없음. 다른 뷰가 테이블을 참조 중이면 에러 발생. (Airflow에서 사전 처리 필요)

<!-- end list -->

```python
# 변환 (Transformation)
df_transformed = df.select(col("Patient").alias("patient"), ...)

# 적재 (Action)
df_transformed.write.mode("overwrite") \
    .jdbc(url=db_url, table=target_table, properties=db_properties)
```

## 4\. Airflow 연동 (SparkSubmitOperator)

Airflow에서 Spark 작업을 실행할 때 사용하는 오퍼레이터.

### 4.1 Deploy Mode: Client vs Cluster

  * **Client Mode:** Driver 프로세스가 Airflow 컨테이너에서 뜸. 로그를 Airflow UI에서 바로 볼 수 있어 디버깅에 유리.
  * **Cluster Mode:** Driver 자체가 Spark Cluster로 넘어가서 실행됨.

### 4.2 네트워크 설정 (Docker 환경 이슈)

Driver(Airflow)와 Worker(Spark)가 서로 다른 컨테이너에 있으므로 통신 설정이 중요함.

  * `spark.master`: Spark Master 서비스 주소 (`spark://spark-master:7077`).
  * `spark.driver.host`: Worker가 Driver(Airflow)에 접속하기 위한 호스트명 (`airflow-scheduler` 서비스명 사용).
  * **호스트명 규칙:** Spark는 호스트명에 언더바(`_`)가 있으면 RFC 위반으로 간주하고 통신을 거부함. (`spark_master` (x) -\> `spark-master` (o))

## 5\. 트러블슈팅 & 배운 점 (삽질 기록)

1.  **의존성 지옥 (Dependency Hell):**

      * Airflow 이미지의 Python 버전과 Spark 라이브러리 버전이 안 맞으면 설치조차 안 됨.
      * 해결: `dbt`는 가상환경(`venv`)으로 격리하고, Airflow Provider는 공식 제약 조건(`constraints`)을 맞춰서 설치.

2.  **Python 경로 불일치:**

      * Driver(Airflow)와 Worker(Spark)의 Python 설치 경로가 다르면 실행 중 에러 발생.
      * 해결: `spark.pyspark.python`, `spark.pyspark.driver.python` 옵션으로 각각의 경로를 명시해 주는 것이 안전함.

3.  **DB Lock (CASCADE) 문제:**

      * Spark `overwrite` 모드는 `DROP TABLE`만 수행함. dbt View가 원본 테이블을 참조 중이면 삭제 실패.
      * 해결: Spark 실행 전 `PostgresOperator`를 이용해 `DROP TABLE ... CASCADE`를 먼저 수행하는 태스크 추가.

4.  **파일 경로 문제:**

      * `client` 모드일지라도 실제 파일 읽기는 Worker가 수행함.
      * Airflow와 Spark Worker 컨테이너 양쪽 모두 **동일한 경로**에 데이터 파일이 마운트되어 있어야 함.