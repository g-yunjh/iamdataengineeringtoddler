# Step 4: Kafka와 Spark Streaming을 이용한 실시간 데이터 처리

4단계의 목표는 멈춰있는 데이터(Batch)가 아니라, 끊임없이 발생하는 \*\*실시간 데이터(Streaming)\*\*를 처리하는 파이프라인을 구축하는 것입니다. 이를 위해 데이터 엔지니어링의 표준인 \*\*Kafka(메시지 브로커)\*\*와 \*\*Spark Structured Streaming(처리 엔진)\*\*을 연동합니다.

## 1\. 아키텍처 이해: Producer와 Consumer

실시간 시스템은 크게 데이터를 **보내는 쪽(Producer)**, **임시로 저장하는 쪽(Broker)**, \*\*가져가서 처리하는 쪽(Consumer)\*\*으로 나뉩니다.

  * **Producer (Python Script):**

      * `patient_treatment.csv` 파일의 데이터를 1초에 한 줄씩 읽어서 마치 실시간으로 환자 이벤트가 발생하는 것처럼 시뮬레이션합니다.
      * 이 데이터를 Kafka의 특정 **Topic**(`patient_events`)으로 전송합니다.

  * **Broker (Apache Kafka & Zookeeper):**

      * **Zookeeper:** Kafka 클러스터의 상태(메타데이터)를 관리하는 코디네이터입니다. Kafka를 띄우기 위해 필수적입니다.
      * **Kafka:** 고성능 메시지 큐입니다. Producer가 보낸 데이터를 Topic이라는 사서함에 순서대로 저장하고 있다가, Consumer가 요청하면 건네줍니다.

  * **Consumer (Spark Structured Streaming):**

      * Kafka에 연결하여 새로운 데이터가 들어올 때마다 즉시 읽어옵니다.
      * 읽어온 데이터를 실시간으로 파싱하고 집계(Aggregation)하여 콘솔에 출력합니다.

## 2\. 환경 구성 (`docker-compose.yml`)

기존 `spark-worker` 서비스 아래에 Kafka 생태계를 추가했습니다.

  * **이미지 선택:** 아키텍처 호환성 문제로 `bitnami` 대신 `confluentinc/cp-kafka`, `cp-zookeeper` 이미지를 사용했습니다. (가장 널리 쓰이는 표준 이미지)
  * **네트워크:** Kafka는 내부 통신용 주소와 외부 통신용 주소 설정(`KAFKA_ADVERTISED_LISTENERS`)이 매우 중요합니다. Docker 네트워크 안에서 통신하므로 `PLAINTEXT://kafka:9092`로 설정했습니다.

## 3\. Producer 구현: `kafka_producer.py`

Python의 `kafka-python` 라이브러리를 사용했습니다.

  * **데이터 전송:** `producer.send(topic, value)` 메서드로 데이터를 보냅니다.
  * **직렬화 (Serialization):** Kafka는 데이터를 바이트(Byte)로만 받습니다. 따라서 Python 딕셔너리(JSON)를 `json.dumps(x).encode('utf-8')`를 통해 바이트로 변환해서 보냈습니다.
  * **키 이슈 해결:** CSV 헤더의 대소문자(`Patient` vs `patient`)와 공백(`  action `) 문제를 해결하기 위해 `df.columns.str.strip()`과 `.get()` 메서드로 안전하게 데이터를 추출했습니다.

## 4\. Consumer 구현: `spark_streaming.py`

Spark의 최신 스트리밍 API인 **Structured Streaming**을 사용했습니다.

### 4.1 SparkSession 생성 및 패키지 로드

```python
spark = SparkSession.builder \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .getOrCreate()
```

  * Kafka와 연결하려면 기본 Spark에는 없는 외부 라이브러리(`spark-sql-kafka`)가 필요합니다. 이를 런타임에 Maven 저장소에서 다운로드하도록 설정했습니다.

### 4.2 ReadStream (Source)

```python
df_raw = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "patient_events") \
    .load()
```

  * `read`가 아닌 \*\*`readStream`\*\*을 사용합니다. 이는 끝이 없는 데이터(Unbounded Table)를 읽겠다는 의미입니다.

### 4.3 데이터 파싱 및 변환 (Transformation)

Kafka에서 들어온 데이터는 `value`라는 컬럼에 바이너리 형태로 들어있습니다.

1.  **CAST:** 바이너리를 문자열(String)로 변환합니다.
2.  **from\_json:** 문자열을 미리 정의한 스키마(`StructType`)에 맞춰 JSON으로 파싱합니다.
3.  **GroupBy:** `action` 컬럼을 기준으로 실시간 발생 건수를 카운트합니다.

### 4.4 WriteStream (Sink)

```python
query = df_count.writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()
```

  * **`outputMode("complete")`:** 데이터가 들어올 때마다 **전체 집계 결과**를 다시 출력합니다.
  * **`format("console")`:** 결과를 터미널 화면에 표 형태로 보여줍니다.
  * **`start()` / `awaitTermination()`:** 스트리밍 쿼리를 시작하고, 강제로 종료하기 전까지 계속 대기합니다.

## 5\. 트러블슈팅 & 배운 점

1.  **이미지 호환성:** `bitnami` 이미지가 특정 환경(WSL2/ARM 등)에서 `latest` 태그를 찾지 못하는 이슈가 있어 `confluentinc` 이미지로 교체하여 해결했습니다.
2.  **들여쓰기(Indentation):** Python 코드를 복사하는 과정에서 공백이 잘못 들어가 `IndentationError`가 발생했습니다. Python 문법의 중요성을 재확인했습니다.
3.  **권한 문제:** Spark가 Kafka 라이브러리를 다운로드할 때 권한(`Permission denied`) 문제가 발생하여 `docker-compose exec --user root` 옵션으로 해결했습니다.
